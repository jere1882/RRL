@article{vvv,
author = {Minniti, Dante and Lucas, P.W. and Emerson, Jim and Saito, R.K. and Hempel, Maren and Pietrukowicz, P. and Ahumada, A. and Alonso, M.V. and Alonso-García, J. and Arias, J. and Bandyopadhyay, Reba and Barbá, Rodolfo and Barbuy, B. and Bedin, L.R. and Bica, E. and Borissova, J. and Bronfman, L. and Carraro, G. and Catelan, Márcio and Zoccali, Manuela},
year = {2010},
month = {12},
pages = {433},
title = {VISTA variables in the via lactea (VVV): The public ESO near-IR variability survey of the Milky Way},
volume = {15},
journal = {New Astronomy},
doi = {10.1016/j.newast.2009.12.002}
}

@book{fisica,
author = {Márcio Catelan, Horace A. Smith},
year = {2015},
title = {Pulsating Stars},
publisher={Wiley-VCH}
}

@BOOK{smith,
       author = {Smith, Horace A.},
        title = "{RR Lyrae Stars}",
         year = 2004,
       adsurl = {https://ui.adsabs.harvard.edu/abs/2004rrls.book.....S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{lopezmartinez2017regularization,
      title={Regularization approaches for support vector machines with applications to biomedical data}, 
      author={Daniel Lopez-Martinez},
      year={2017},
      eprint={1710.10600},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{l1l2,
author = {Koshiba, Y. and Abe, Shigeo},
year = {2003},
month = {08},
pages = {2054 - 2059 vol.3},
title = {Comparison of L1 and L2 Support Vector Machines},
volume = {3},
isbn = {0-7803-7898-9},
doi = {10.1109/IJCNN.2003.1223724}
}

@article{vvv_actual,
author = {Catelan, Márcio and Dekany, Istvan and Hempel, Maren and Minniti, Dante},
year = {2014},
month = {06},
pages = {},
title = {Stellar Variability in the VVV Survey: An Update}
}

@book{statisticallearning,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

@book{c45,
author = {Quinlan, J. Ross},
title = {C4.5: Programs for Machine Learning},
year = {1993},
isbn = {1558602380},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA}
}

@electronic{noauthororeditor2013applied,
  added-at = {2015-05-25T13:54:39.000+0200},
  address = {New York, NY},
  author = {Kuhn, Max and Johnson, Kjell},
  biburl = {https://www.bibsonomy.org/bibtex/20ffe1d130cc2d823a0d83058c337dd8d/vivion},
  booktitle = {Applied Predictive Modeling},
  description = {Applied Predictive Modeling: 9781461468486: Medicine & Health Science Books @ Amazon.com},
  interhash = {28fb1de6e768ed47c29f0a5d6d6dbe5d},
  intrahash = {0ffe1d130cc2d823a0d83058c337dd8d},
  isbn = {9781461468493 1461468493 1461468485 9781461468486},
  keywords = {ML data-science machine-learning statistics},
  publisher = {Springer},
  refid = {844349710},
  timestamp = {2015-05-25T14:06:06.000+0200},
  title = {Applied predictive modeling},
  url = {http://www.amazon.com/Applied-Predictive-Modeling-Max-Kuhn/dp/1461468485/},
  year = 2013
}

@article{breiman96,
  added-at = {2011-05-12T20:52:32.000+0200},
  author = {Breiman, Leo},
  biburl = {https://www.bibsonomy.org/bibtex/2d7f2fcb651038d60087aa9d4a046eef8/smolav},
  interhash = {96b419212a439d785711b0d79e00332d},
  intrahash = {d7f2fcb651038d60087aa9d4a046eef8},
  journal = {Machine Learning},
  keywords = {bagging ensemble in-thesis machine-learning projn2011},
  number = 2,
  pages = {123-140},
  timestamp = {2011-07-28T23:34:55.000+0200},
  title = {Bagging Predictors},
  volume = 24,
  year = 1996
}







@article{id3,
author = {Quinlan, J. R.},
title = {Induction of Decision Trees},
year = {1986},
issue_date = {March 1986},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {1},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1022643204877},
doi = {10.1023/A:1022643204877},
abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
journal = {Mach. Learn.},
month = mar,
pages = {81–106},
numpages = {26},
keywords = {knowledge acquisition, information theory, decision trees, classification, expert systems, induction}
}


@article{svm_practical,
author = {Hsu, Chih-wei and Chang, Chih-chung and Lin, Chih-Jen},
year = {2003},
month = {11},
pages = {},
title = {A Practical Guide to Support Vector Classification Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin}
}

@article{svm_importance,
author = {Rakotomamonjy, Alain},
title = {Variable Selection Using Svm Based Criteria},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1357–1370},
numpages = {14}
}
@article{rf_collinearity,
author = {Kuperman, Victor and Matsuki, Kazunaga and Van Dyke, Julie},
year = {2016},
month = {12},
pages = {},
title = {The Random Forests statistical technique: An examination of its value for the study of reading},
volume = {20},
journal = {Scientific Studies of Reading},
doi = {10.1080/10888438.2015.1107073}
}

@article{ginigain,
author = {Raileanu, Laura and Stoffel, Kilian},
year = {2004},
month = {05},
pages = {77-93},
title = {Theoretical Comparison between the Gini Index and Information Gain Criteria},
volume = {41},
journal = {Annals of Mathematics and Artificial Intelligence},
doi = {10.1023/B:AMAI.0000018580.96245.c6}
}

@article{how_to_choose_correlation,
author = {Harry Khamis},
title ={Measures of Association: How to Choose?},
journal = {Journal of Diagnostic Medical Sonography},
volume = {24},
number = {3},
pages = {155-162},
year = {2008},
doi = {10.1177/8756479308317006}
}

@article{kendall_spearman,
title = {Effective use of Spearman's and Kendall's correlation coefficients for association between two measured traits},
journal = {Animal Behaviour},
volume = {102},
pages = {77-84},
year = {2015},
issn = {0003-3472},
doi = {https://doi.org/10.1016/j.anbehav.2015.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0003347215000196},
author = {Marie-Therese Puth and Markus Neuhäuser and Graeme D. Ruxton},
keywords = {confidence interval, null hypothesis testing, Pearson's product–moment correlation coefficient, power, statistics, type 1 error},
abstract = {We examine the performance of the two rank order correlation coefficients (Spearman's rho and Kendall's tau) for describing the strength of association between two continuously measured traits. We begin by discussing when these measures should, and should not, be preferred over Pearson's product–moment correlation coefficient on conceptual grounds. For testing the null hypothesis of no monotonic association, our simulation studies found both rank coefficients show similar performance to variants of the Pearson product–moment measure of association, and provide only slightly better performance than Pearson's measure even if the two measured traits are non-normally distributed. Where variants of the Pearson measure are not appropriate, there was no strong reason (based on our results) to select either of our rank-based alternatives over the other for testing the null hypothesis of no monotonic association. Further, our simulation studies indicated that for both rank coefficients there exists at least one method for calculating confidence intervals that supplies results close to the desired level if there are no tied values in the data. In this case, Kendall's coefficient produces consistently narrower confidence intervals, and might thus be preferred on that basis. However, if there are any ties in the data, irrespective of whether the percentage of ties is small or large, Spearman's measure returns values closer to the desired coverage rates, whereas Kendall's results differ more and more from the desired level as the number of ties increases, especially for large correlation values.}
}
@book{pagano2010understanding,
  title={Understanding Statistics in the Behavioral Sciences},
  author={Pagano, R.R.},
  isbn={9780495596578},
  lccn={2008937835},
  url={https://books.google.li/books?id=tinxPAAACAAJ},
  year={2010},
  publisher={Wadsworth, Cengage Learning}
}

@article{offshelf,
author = {Wyner, Abraham and Olson, Matthew and Bleich, Justin and Mease, David},
year = {2015},
month = {04},
pages = {},
title = {Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers},
volume = {18},
journal = {Journal of Machine Learning Research}
}

@inproceedings{NIPS2012_621bf66d,
 author = {Yang, Tianbao and Li, Yu-feng and Mahdavi, Mehrdad and Jin, Rong and Zhou, Zhi-Hua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Nystr\"{o}m Method vs Random Fourier Features: A Theoretical and Empirical Comparison},
 url = {https://proceedings.neurips.cc/paper/2012/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf},
 volume = {25},
 year = {2012}
}

@inproceedings{NIPS2007_013a006f,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2008}
}


@book{han2012mining,
  address = {Waltham, Mass.},
  author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
  biburl = {https://www.bibsonomy.org/bibtex/2beb274b9aeaebb87f5423781b6839f54/hotho},
  description = {Data Mining: Concepts and Techniques (Morgan Kaufmann Series in Data Management Systems): Amazon.de: Jiawei Han, Micheline Kamber, Jian Pei: Englische Bücher},
  interhash = {247a70f1f22ce1914e46d7ff6f43e378},
  intrahash = {beb274b9aeaebb87f5423781b6839f54},
  isbn = {0123814790},
  keywords = {data mining tobuy},
  publisher = {Morgan Kaufmann Publishers},
  refid = {818321921},
  timestamp = {2013-04-15T17:17:42.000+0200},
  title = {Data mining concepts and techniques, third edition},
  url = {http://www.amazon.de/Data-Mining-Concepts-Techniques-Management/dp/0123814790/ref=tmm_hrd_title_0?ie=UTF8&qid=1366039033&sr=1-1},
  year = 2012
}

@book{191611,
	author = {Moore, David S. and McCabe, George P.},
	title = {Introduction to the practice of statistics /},
	publisher = {W.H. Freeman,},
	year = {c1989.},
	address = {New York :}
}

@article{KRZYSZTOFOWICZ1997286,
title = {Transformation and normalization of variates with specified distributions},
journal = {Journal of Hydrology},
volume = {197},
number = {1},
pages = {286-292},
year = {1997},
issn = {0022-1694},
doi = {https://doi.org/10.1016/S0022-1694(96)03276-3},
url = {https://www.sciencedirect.com/science/article/pii/S0022169496032763},
author = {Roman Krzysztofowicz},
abstract = {Given two continuous random variables X and Y, with specified strictly increasing cumulative distribution functions F and G, respectively, the one-to-one transform t which maps one variate into another, say Y=t(X), has an analytic form, t(X)=G−1(F(X)) or t(X)=G−1(1−F(X)), depending upon whether t is increasing or decreasing. This fact of probability theory is reviewed and compared with another method for finding t that was recently proposed. Applications to system identification, normalization of a variate, and normalization of a sample are briefly discussed.}
}

@article{mutual_info,
author = {Kraskov, Alexander and Stogbauer, Harald and Grassberger, Peter},
year = {2004},
month = {07},
pages = {066138},
title = {Estimating Mutual Information},
volume = {69},
journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},
doi = {10.1103/PhysRevE.69.066138}
}

@TECHREPORT{carreira,
    author = {Miguel A. Carreira-perpinan},
    title = {A review of dimension reduction techniques},
    institution = {},
    year = {1997}
}

@inbook{duda,
author = {Duda, Richard and Hart, Peter and G.Stork, David},
year = {2001},
month = {01},
pages = {},
title = {Pattern Classification},
volume = {xx},
isbn = {0-471-05669-3},
journal = {Wiley Interscience}
}

@inbook{clustering,
author = {Rokach, Lior and Maimon, Oded},
year = {2005},
month = {01},
pages = {321-352},
title = {Clustering Methods},
journal = {Data Mining and Knowledge Discovery Handbook},
doi = {10.1007/0-387-25465-X_15}
}

@book{Jain88,
  added-at = {2011-03-22T22:55:58.000+0100},
  address = {Upper Saddle River, NJ, USA},
  author = {Jain, Anil K. and Dubes, Richard C.},
  biburl = {https://www.bibsonomy.org/bibtex/24a1adbfdc7b83b201dd8fb3e5a109609/ans},
  description = {graph mining},
  file = {jain1988algorithms.pdf:jain1988algorithms.pdf:PDF},
  interhash = {443a79c152c5681cdc664714b50d116c},
  intrahash = {4a1adbfdc7b83b201dd8fb3e5a109609},
  keywords = {algorithms clustering graph master},
  lastdatemodified = {2007-03-13},
  lastname = {Jain},
  own = {notown},
  pdf = {jain88_algorithms.pdf},
  publisher = {Prentice-Hall, Inc.},
  read = {notread},
  timestamp = {2011-03-22T23:02:17.000+0100},
  title = {Algorithms for clustering data},
  url = {http://portal.acm.org/citation.cfm?id=46712},
  year = 1988
}

@book{iretrieval,
author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
title = {Introduction to Information Retrieval},
year = {2008},
isbn = {0521865719},
publisher = {Cambridge University Press},
address = {USA},
abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.}
}

@article{ward,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2282967},
 abstract = {A procedure for forming hierarchical groups of mutually exclusive subsets, each of which has members that are maximally similar with respect to specified characteristics, is suggested for use in large-scale ($n > 100$) studies when a precise optimal solution for a specified number of groups is not practical. Given n sets, this procedure permits their reduction to n - 1 mutually exclusive sets by considering the union of all possible n(n - 1)/2 pairs and selecting a union having a maximal value for the functional relation, or objective function, that reflects the criterion chosen by the investigator. By repeating this process until only one group remains, the complete hierarchical structure and a quantitative estimate of the loss associated with each stage in the grouping can be obtained. A general flowchart helpful in computer programming and a numerical example are included.},
 author = {Joe H. Ward},
 journal = {Journal of the American Statistical Association},
 number = {301},
 pages = {236--244},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Hierarchical Grouping to Optimize an Objective Function},
 volume = {58},
 year = {1963}
}


@article{fextraction,
author = {Ghojogh, Benyamin and Samad, Maria and Mashhadi, Sayema and Kapoor, Tania and Ali, Wahab and Karray, Fakhri and Crowley, Mark},
year = {2019},
month = {05},
pages = {},
title = {Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review}
}

@article{fs1,
author = {Liu, Huan and Motoda, Hiroshi and Setiono, Rudy and Zhao, Zheng},
year = {2010},
month = {01},
pages = {4-13},
title = {Feature Selection: An Ever Evolving Frontier in Data Mining.},
volume = {10},
journal = {Journal of Machine Learning Research - Proceedings Track}
}

@article{fs2,
title = {Feature selection for classification},
journal = {Intelligent Data Analysis},
volume = {1},
number = {1},
pages = {131-156},
year = {1997},
issn = {1088-467X},
doi = {https://doi.org/10.1016/S1088-467X(97)00008-5},
url = {https://www.sciencedirect.com/science/article/pii/S1088467X97000085},
author = {M. Dash and H. Liu},
keywords = {Feature selection, Classification, Framework},
abstract = {Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications.}
}

@article{fs3,
author = {Guyon, Isabelle and Elisseeff, Andr\'{e}},
title = {An Introduction to Variable and Feature Selection},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1157–1182},
numpages = {26}
}

@inproceedings{fs4,
author = {Jovic, Alan and Brkić, Karla and Bogunovic, N.},
year = {2015},
month = {05},
pages = {1200-1205},
title = {A review of feature selection methods with applications},
doi = {10.1109/MIPRO.2015.7160458}
}

@article{Benjamin_2003,
   title={GLIMPSE. I. AnSIRTFLegacy Project to Map the Inner Galaxy},
   volume={115},
   ISSN={1538-3873},
   url={http://dx.doi.org/10.1086/376696},
   DOI={10.1086/376696},
   number={810},
   journal={Publications of the Astronomical Society of the Pacific},
   publisher={IOP Publishing},
   author={Benjamin, Robert A. and Churchwell, E. and Babler, Brian L. and Bania, T. M. and Clemens, Dan P. and Cohen, Martin and Dickey, John M. and Indebetouw, Rémy and Jackson, James M. and Kobulnicky, Henry A. and et al.},
   year={2003},
   month={Aug},
   pages={953–964}
}

@article{Skrutskie_2006,
	doi = {10.1086/498708},
	url = {https://doi.org/10.1086/498708},
	year = 2006,
	month = {feb},
	publisher = {American Astronomical Society},
	volume = {131},
	number = {2},
	pages = {1163--1183},
	author = {M. F. Skrutskie and R. M. Cutri and R. Stiening and M. D. Weinberg and S. Schneider and J. M. Carpenter and C. Beichman and R. Capps and T. Chester and J. Elias and J. Huchra and J. Liebert and C. Lonsdale and D. G. Monet and S. Price and P. Seitzer and T. Jarrett and J. D. Kirkpatrick and J. E. Gizis and E. Howard and T. Evans and J. Fowler and L. Fullmer and R. Hurt and R. Light and E. L. Kopan and K. A. Marsh and H. L. McCallon and R. Tam and S. Van Dyk and S. Wheelock},
	title = {The Two Micron All Sky Survey (2MASS)},
	journal = {The Astronomical Journal},
	abstract = {Between 1997 June and 2001 February the Two Micron All Sky Survey (2MASS) collected 25.4 Tbytes of raw imaging data covering 99.998% of the celestial sphere in the near-infrared J (1.25 μm), H (1.65 μm), and Ks (2.16 μm) bandpasses. Observations were conducted from two dedicated 1.3 m diameter telescopes located at Mount Hopkins, Arizona, and Cerro Tololo, Chile. The 7.8 s of integration time accumulated for each point on the sky and strict quality control yielded a 10 σ point-source detection level of better than 15.8, 15.1, and 14.3 mag at the J, H, and Ks bands, respectively, for virtually the entire sky. Bright source extractions have 1 σ photometric uncertainty of <0.03 mag and astrometric accuracy of order 100 mas. Calibration offsets between any two points in the sky are <0.02 mag. The 2MASS All-Sky Data Release includes 4.1 million compressed FITS images covering the entire sky, 471 million source extractions in a Point Source Catalog, and 1.6 million objects identified as extended in an Extended Source Catalog.}
}

@article{ej1,
author = {Debosscher, Jonas and Sarro, Luis and Aerts, C. and Cuypers, Jan and Vandenbussche, Bart and Garrido, Rafael and Solano, E.},
year = {2007},
month = {11},
pages = {},
title = {Automated supervised classification of variable stars I. Methodology},
volume = {475},
journal = {Astronomy and Astrophysics},
doi = {10.1051/0004-6361:20077638}
}

@article{ej2,
author = {Dubath, Pierre and Rimoldini, L. and Süveges, Maria and Blomme, Jonas and López, M and Sarro, Luis and De Ridder, J. and Cuypers, Jan and Guy, Leanne and Lecoeur, I. and Nienartowicz, K. and Jan, A. and Beck, Mathias and Mowlavi, N. and Cat, P. and Lebzelter, Thomas and Eyer, L.},
year = {2011},
month = {01},
pages = {},
title = {Random forest automated supervised classification of Hipparcos periodic
variable stars},
volume = {414},
journal = {Monthly Notices of The Royal Astronomical Society - MON NOTIC ROY ASTRON SOC},
doi = {10.1111/j.1365-2966.2011.18575.x}
}


@article{ej4,
author = {Paegert, Martin and Stassun, Keivan and Burger, Dan},
year = {2014},
month = {06},
pages = {31},
title = {The EB Factory Project I. A Fast, Neural Net Based, General Purpose Light Curve Classifier Optimized for Eclipsing Binaries},
volume = {148},
journal = {The Astronomical Journal},
doi = {10.1088/0004-6256/148/2/31}
}

@article{ej3,
author = {Armstrong, D. and Kirk, J. and Lam, K. and McCormac, J. and Walker, S. and Brown, D. and Osborn, Hugh and Pollacco, D. and Spake, J.},
year = {2015},
month = {02},
pages = {},
title = {K2 Variable Catalogue: Variable Stars and Eclipsing Binaries in K2 Campaigns 1 and 0},
journal = {Astronomy and Astrophysics},
doi = {10.1051/0004-6361/201525889}
}


@article{elorrieta,
author = {Elorrieta López, Felipe and Eyheramendy, Susana and Jordán, Andrés and Dekany, Istvan},
year = {2016},
month = {09},
pages = {},
title = {A machine learned classifier for RR Lyrae in the VVV survey},
volume = {595},
journal = {Astronomy \& Astrophysics},
doi = {10.1051/0004-6361/201628700}
}

@article{friedman2000greedy,
  added-at = {2012-10-21T11:58:58.000+0200},
  author = {Friedman, Jerome H.},
  biburl = {https://www.bibsonomy.org/bibtex/237ca72b4c7f9383050b7c50da4356802/nosebrain},
  interhash = {11df73eca7ccfc0e74d70d3de429965c},
  intrahash = {37ca72b4c7f9383050b7c50da4356802},
  journal = {Annals of Statistics},
  keywords = {MART approximation boosting function greedy},
  pages = {1189--1232},
  timestamp = {2012-10-21T12:01:26.000+0200},
  title = {Greedy Function Approximation: A Gradient Boosting Machine},
  volume = 29,
  year = 2000
}

@article{friedman2,
author = {Friedman, Jerome H.},
title = {Stochastic Gradient Boosting},
year = {2002},
issue_date = {28 February 2002},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {38},
number = {4},
issn = {0167-9473},
url = {https://doi.org/10.1016/S0167-9473(01)00065-2},
doi = {10.1016/S0167-9473(01)00065-2},
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current "pseudo'-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
journal = {Comput. Stat. Data Anal.},
month = feb,
pages = {367–378},
numpages = {12}
}

@misc{louppe2015understanding,
      title={Understanding Random Forests: From Theory to Practice}, 
      author={Gilles Louppe},
      year={2015},
      eprint={1407.7502},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@ARTICLE{jbc,
       author = {{Cabral}, Juan B. and {Ramos}, Felipe and {Gurovich}, Sebasti{\'a}n and
         {Granitto}, Pablo},
        title = "{Automatic Catalog of RRLyrae from $\sim$ 14 million VVV Light Curves: How far can we go with traditional machine-learning?}",
      journal = {arXiv e-prints},
     keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Astrophysics - Solar and Stellar Astrophysics, Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = may,
          eid = {arXiv:2005.00220},
        pages = {arXiv:2005.00220},
archivePrefix = {arXiv},
       eprint = {2005.00220},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200500220C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Shapley,
       author = {Shapley, H.},
        title = "Studies based on the colors and magnitudes in stellar clusters. VII. The distances, distribution in space, and dimensions of 69 globular clusters.",
         year = 1918,
        month = oct,
       volume = {48},
        pages = {154-181},
          doi = {10.1086/142423},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1918ApJ....48..154S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{adaboost,
title = "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting",
journal = "JournalSamus, N. N., Durlevich, O. V., et al of Computer and System Sciences",
volume = "55",
number = "1",
pages = "119 - 139",
year = "1997",
issn = "0022-0000",
doi = "https://doi.org/10.1006/jcss.1997.1504",
url = "http://www.sciencedirect.com/science/article/pii/S002200009791504X",
author = "Yoav Freund and Robert E Schapire",
}


@article{rf,
author = {Breiman, L},
year = {2001},
month = {10},
pages = {5-32},
title = {Random Forests},
volume = {45},
journal = {Machine Learning},
doi = {10.1023/A:1010950718922}
}

@article{fsiher,
author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
year = {2018},
month = {01},
pages = {},
title = {Model Class Reliance: Variable Importance Measures for any Machine Learning Model Class, from the "Rashomon" Perspective}
}

@article{svm,
author = {Boser, Bernhard and Guyon, Isabelle and Vapnik, Vladimir},
year = {1996},
month = {08},
pages = {},
title = {A Training Algorithm for Optimal Margin Classifier},
volume = {5},
journal = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
doi = {10.1145/130385.130401}
}

@article{svm2,
author = {Cortes, Corinna and Vapnik, Vladimir},
title = {Support-Vector Networks},
year = {1995},
issue_date = {Sept. 1995},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1022627411411},
doi = {10.1023/A:1022627411411},
abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
journal = {Mach. Learn.},
month = sep,
pages = {273–297},
numpages = {25},
keywords = {efficient learning algorithms, pattern recognition, radial basis function classifiers, polynomial classifiers, neural networks}
}



@book{knn,
author = {Vapnik, Vladimir},
year = {2000},
title = {The Nature of Statistical Learning Theory},
publisher={McGraw-Hill}
}

@article{vapnik71uniform,
  abstract = {the VC dimension},
  added-at = {2007-09-05T18:38:03.000+0200},
  author = {Vapnik, V. N. and Chervonenkis, A. Ya.},
  biburl = {https://www.bibsonomy.org/bibtex/268efd7e1e848c6ac4c7af34f7a7529d8/sb3000},
  interhash = {ea0691b944645bdace30d43c16914aa3},
  intrahash = {68efd7e1e848c6ac4c7af34f7a7529d8},
  journal = {Theory of Probability and its Applications},
  keywords = {learning-theory svm kernel},
  number = 2,
  pages = {264--280},
  timestamp = {2010-10-07T14:13:57.000+0200},
  title = {On the Uniform Convergence of Relative Frequencies of Events to their Probabilities},
  volume = 16,
  year = 1971
}

@book{vapnik74theory,
  abstract = {In these two books, the original &quot;Generalized Portrait&quot;
    Algorithm for constructing separating hyperplanes with optimal margin
    is described.},
  added-at = {2008-04-30T12:59:47.000+0200},
  address = {Moscow},
  author = {Vapnik, V. and Chervonenkis, A.},
  biburl = {https://www.bibsonomy.org/bibtex/29a33742ce9e3e4df8e4691f8e4c75e3a/kdubiq},
  description = {KDubiq Blueprint},
  groupsearch = {0},
  interhash = {936f556afc966ddda07ba175241d6924},
  intrahash = {9a33742ce9e3e4df8e4691f8e4c75e3a},
  keywords = {imported},
  note = {(German Translation: W.~Wapnik \& A.~Tscherwonenkis, {\em Theorie
    der Zeichenerkennung}, Akademie--Verlag, Berlin, 1979)},
  publisher = {Nauka},
  timestamp = {2008-04-30T13:00:30.000+0200},
  title = {Theory of Pattern Recognition },
  year = 1974
}


@book{slearning,
  added-at = {2019-10-12T20:03:56.000+0200},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  biburl = {https://www.bibsonomy.org/bibtex/2444186c86d18bddb4433c12fa126f6be/lopusz_kdd},
  interhash = {b3febabdc45a8629023cee7323dfbd86},
  intrahash = {444186c86d18bddb4433c12fa126f6be},
  keywords = {general_machine_learning},
  publisher = {Springer},
  timestamp = {2019-10-12T23:45:37.000+0200},
  title = {An Introduction to Statistical Learning: with Applications in R },
  url = {https://faculty.marshall.usc.edu/gareth-james/ISL/},
  year = 2013
}

@inbook{cross_validation,
author = {Berrar, Daniel},
year = {2018},
month = {01},
pages = {},
title = {Cross-Validation},
isbn = {9780128096338},
doi = {10.1016/B978-0-12-809633-8.20349-X}
}

@ARTICLE{catalogo,
       author = {{Samus}, N.~N. and {Kazarovets}, E.~V. and {Durlevich}, O.~V. and {Kireeva}, N.~N. and {Pastukhova}, E.~N.},
        title = "{VizieR Online Data Catalog: General Catalogue of Variable Stars (Samus+, 2007-2017)}",
      journal = {VizieR Online Data Catalog},
     keywords = {Stars: variable, Combined data},
         year = 2009,
        month = jan,
          eid = {B/gcvs},
        pages = {B/gcvs},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2009yCat....102025S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}




@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}

@article{liblinear,
  added-at = {2009-07-08T16:22:45.000+0200},
  address = {Cambridge, MA, USA},
  author = {Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
  biburl = {https://www.bibsonomy.org/bibtex/2834ebb2a6b98ab0156d1faffa08cd302/zeno},
  interhash = {02cc46e508ee2ea9690079cb40a400d7},
  intrahash = {834ebb2a6b98ab0156d1faffa08cd302},
  issn = {1533-7928},
  journal = {Journal of Machine Learning Research},
  keywords = {imported},
  publisher = {MIT Press},
  timestamp = {2009-07-08T16:22:46.000+0200},
  title = {LIBLINEAR: A Library for Large Linear Classification},
  volume = 9,
  year = 2008
}

@article{libsvm,
  added-at = {2011-12-06T10:58:14.000+0100},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  biburl = {https://www.bibsonomy.org/bibtex/22442e014045d31ddd9e9000d5b214b67/utahell},
  description = {LIBSVM FAQ},
  interhash = {b6d3cf7743c5d218d5bc5f789e3a9984},
  intrahash = {2442e014045d31ddd9e9000d5b214b67},
  issue = {3},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  keywords = {libsvm svm},
  note = {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}},
  pages = {27:1--27:27},
  timestamp = {2011-12-16T16:26:02.000+0100},
  title = {{LIBSVM}: A Library for Support Vector Machines},
  volume = 2,
  year = 2011
}

@article{he,
author = {He, Haibo and Garcia, E.A.},
year = {2009},
month = {10},
pages = {1263 - 1284},
title = {Learning from Imbalanced Data},
volume = {21},
journal = {Knowledge and Data Engineering, IEEE Transactions on},
doi = {10.1109/TKDE.2008.239}
}

@article{Menardi2012TrainingAA,
  title={Training and assessing classification rules with imbalanced data},
  author={G. Menardi and N. Torelli},
  journal={Data Mining and Knowledge Discovery},
  year={2012},
  volume={28},
  pages={92-122}
}

@article{smote,
   title={SMOTE: Synthetic Minority Over-sampling Technique},
   volume={16},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.953},
   DOI={10.1613/jair.953},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
   year={2002},
   month={Jun},
   pages={321–357}
}

@book{archdeacon1994correlation,
  title={Correlation and Regression Analysis: A Historian's Guide},
  author={Archdeacon, T.J.},
  isbn={9780299136505},
  lccn={92056927},
  url={https://books.google.com/books?id=tptmAAAAMAAJ},
  year={1994},
  publisher={University of Wisconsin Press}
}

@inproceedings{adasyn,
author = {He, Haibo and Bai, Yang and Garcia, Edwardo and Li, Shutao},
year = {2008},
month = {07},
pages = {1322 - 1328},
title = {ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning},
journal = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2008.4633969}
}

@inproceedings{Batista2003BalancingTD,
  title={Balancing Training Data for Automated Annotation of Keywords: a Case Study},
  author={Gustavo E. A. P. A. Batista and A. Bazzan and M. C. Monard},
  booktitle={WOB},
  year={2003}
}

@article{nathalie,
author = {Japkowicz, Nathalie},
year = {2000},
month = {06},
pages = {},
title = {The Class Imbalance Problem: Significance and Strategies},
journal = {Proceedings of the 2000 International Conference on Artificial Intelligence ICAI}
}

@inproceedings{nystroem,
 author = {Williams, Christopher and Seeger, Matthias},
 booktitle = {Advances in Neural Information  The relationship between infrared, optical, and ultraviolet extinction.Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {682--688},
 publisher = {MIT Press},
 title = {Using the Nystr\"{o}m Method to Speed Up Kernel Machines},
 url = {https://proceedings.neurips.cc/paper/2000/file/19de10adbaa1b2ee13f77f679fa1483a-Paper.pdf},
 volume = {13},
 year = {2001}
}

@inproceedings{imbalanced_svm,
author = {Akbani, Rehan and Kwek, Stephen and Japkowicz, Nathalie},
year = {2004},
month = {09},
pages = {39-50},
title = {Applying Support Vector Machines to Imbalanced Data Sets},
volume = {3201},
isbn = {978-3-540-23105-9},
journal = {Lecture Notes Artif. Intell.},
doi = {10.1007/978-3-540-30115-8_7}
}

@INPROCEEDINGS{wu-chang,
    author = {Gang Wu and Edward Y. Chang},
    title = {Class-boundary alignment for imbalanced dataset learning},
    booktitle = {In ICML 2003 Workshop on Learning from Imbalanced Data Sets},
    year = {2003},
    pages = {49--56}
}


@article{yeo,
    author = {Yeo, In‐Kwon and Johnson, Richard A.},
    title = "{A new family of power transformations to improve normality or symmetry}",
    journal = {Biometrika},
    volume = {87},
    number = {4},
    pages = {954-959},
    year = {2000},
    month = {12},
    abstract = "{We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box–Cox transformation for positive variables. The large‐sample properties of the transformation are investigated in the contect of a single random sample.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/87.4.954},
    url = {https://doi.org/10.1093/biomet/87.4.954},
    eprint = {https://academic.oup.com/biomet/article-pdf/87/4/954/633221/870954.pdf}
}


@article{bailey,
    author = {Bailey, S. I.},
    title = "{A discussion of variable stars in the cluster Omega Centauri}",
    journal = {Annals of the Astronomical Observatory of Harvard College},
    volume = {38},
    year = {1902}
}

@article{gran1,
   title={Bulge RR Lyrae stars in the VVV tile b201},
   volume={575},
   ISSN={1432-0746},
   url={http://dx.doi.org/10.1051/0004-6361/201424333},
   DOI={10.1051/0004-6361/201424333},
   journal={Astronomy and Astrophysics},
   publisher={EDP Sciences},
   author={Gran, F. and Minniti, D. and Saito, R. K. and Navarrete, C. and Dékány, I. and McDonald, I. and Contreras Ramos, R. and Catelan, M.},
   year={2015},
   month={Mar},
   pages={A114}
}

@article{gran2,
   title={Mapping the outer bulge with RRab stars from the VVV Survey},
   volume={591},
   ISSN={1432-0746},
   url={http://dx.doi.org/10.1051/0004-6361/201527511},
   DOI={10.1051/0004-6361/201527511},
   journal={Astronomy and Astrophysics},
   publisher={EDP Sciences},
   author={Gran, F. and Minniti, D. and Saito, R. K. and Zoccali, M. and Gonzalez, O. A. and Navarrete, C. and Catelan, M. and Contreras Ramos, R. and Elorrieta, F. and Eyheramendy, S. and et al.},
   year={2016},
   month={Jul},
   pages={A145}
}

@article{ogle,
author = {Soszyński, I. and Udalski, A. and Wrona, Marcin and Szymański, M. and Pietrukowicz, P. and Skowron, J. and Skowron, D. and Poleski, R. and Kozlowski, Szymon and Mróz, P. and Ulaczyk, K. and Rybicki, K. and Iwanek, P. and Gromadzki, Mariusz},
year = {2019},
month = {12},
pages = {},
title = {Over 78 000 RR Lyrae Stars in the Galactic Bulge and Disk from the OGLE Survey}
}

@article{gaia,
   title={Gaia Data Release 2},
   volume={616},
   ISSN={1432-0746},
   url={http://dx.doi.org/10.1051/0004-6361/201833051},
   DOI={10.1051/0004-6361/201833051},
   journal={Astronomy and Astrophysics},
   publisher={EDP Sciences},
   author={Brown, A. G. A. and Vallenari, A. and Prusti, T. and de Bruijne, J. H. J. and Babusiaux, C. and Bailer-Jones, C. A. L. and Biermann, M. and Evans, D. W. and Eyer, L. and et al.},
   year={2018},
   month={Aug},
   pages={A1}
}

@article{Richards_2011,
   title={On machine-learned classification of variable starts with sparse and noisy time-series data.},
   volume={733},
   ISSN={1538-4357},
   url={http://dx.doi.org/10.1088/0004-637X/733/1/10},
   DOI={10.1088/0004-637x/733/1/10},
   number={1},
   journal={The Astrophysical Journal},
   publisher={American Astronomical Society},
   author={Richards, Joseph W. and Starr, Dan L. and Butler, Nathaniel R. and Bloom, Joshua S. and Brewer, John M. and Crellin-Quick, Arien and Higgins, Justin and Kennedy, Rachel and Rischard, Maxime},
   year={2011},
   month={Apr},
   pages={10}
}

@article{303d9825f1794a8b86d692ee85c9a602,
title = "Data mining and machine learning in astronomy",
abstract = "We review the current state of data mining and machine learning in astronomy. Data Mining can have a somewhat mixed connotation from the point of view of a researcher in this field. If used correctly, it can be a powerful approach, holding the potential to fully exploit the exponentially increasing amount of available data, promising great scientific advance. However, if misused, it can be little more than the black box application of complex computing algorithms that may give little physical insight, and provide questionable results. Here, we give an overview of the entire data mining process, from data collection through to the interpretation of results. We cover common machine learning algorithms, such as artificial neural networks and support vector machines, applications from a broad range of astronomy, emphasizing those in which data mining techniques directly contributed to improving science, and important current and future directions, including probability density functions, parallel algorithms, Peta-Scale computing, and the time domain. We conclude that, so long as one carefully selects an appropriate algorithm and is guided by the astronomical problem at hand, data mining can be very much the powerful tool, and not the questionable black box.",
keywords = "Data mining, Virtual Observatory, astroinformatics, astrostatistics, knowledge discovery in databases, machine learning",
author = "Ball, {Nicholas M.} and Brunner, {Robert J.}",
year = "2010",
month = jul,
doi = "10.1142/S0218271810017160",
language = "English (US)",
volume = "19",
pages = "1049--1106",
journal = "International Journal of Modern Physics D",
issn = "0218-2718",
publisher = "World Scientific Publishing Co. Pte Ltd",
number = "7",
}

@article{schwarzschild,
       author = {Schwarzchild, Martin.},
        title = "{On the Light Curve of Delta Cephei.}",
      journal = {Harvard College Observatory Circular},
         year = 1938,
        month = dec,
       volume = {431},
        pages = {1-13},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1938HarCi.431....1S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{emerson,
author = {Emerson, Jim and Irwin, Mike and Lewis, Jim and Hodgkin, Simon and Evans, Dafydd and Bunclark, Peter and Mcmahon, Richard and Hambly, Nathan and Mann, Robert and Bond, Ian and Sutorius, Eckhard and Read, Michael and Williams, Peredur and Lawrence, Andrew and Stewart, Malcolm},
year = {2004},
month = {09},
pages = {},
title = {VISTA data flow system: overview},
volume = {5493},
journal = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.551582}
}

@article{cross,
author = {Gray, Jim and Szalay, Alexander and Budavari, Tamas and Lupton, Robert and Nieto-Santisteban, María and Thakar, Ani},
year = {2007},
month = {01},
pages = {},
title = {Cross-Matching Multiple Spatial Observations and Dealing with Missing Data},
volume = {abs/cs/0701172},
journal = {CoRR}
}

@article{eastman,
   title={Achieving Better Than 1 Minute Accuracy in the Heliocentric and Barycentric Julian Dates},
   volume={122},
   ISSN={1538-3873},
   url={http://dx.doi.org/10.1086/655938},
   DOI={10.1086/655938},
   number={894},
   journal={Publications of the Astronomical Society of the Pacific},
   publisher={IOP Publishing},
   author={Eastman, Jason and Siverd, Robert and Gaudi, B. Scott},
   year={2010},
   month={Aug},
   pages={935–946}
}

@article{Lomb:1976wy,
    author = "Lomb, N. R.",
    title = "{Least - squares frequency analysis of unequally spaced data}",
    doi = "10.1007/BF00648343",
    journal = "Astrophys. Space Sci.",
    volume = "39",
    pages = "447--462",
    year = "1976"
}

@article{scargle,
author = {Scargle, Jeffrey},
year = {1983},
month = {01},
pages = {},
title = {Studies in astronomical time series analysis. II - Statistical aspects of spectral analysis of unevenly spaced data},
volume = {263},
journal = {The Astrophysical Journal},
doi = {10.1086/160554}
}

@article{VanderPlas_2018,
	doi = {10.3847/1538-4365/aab766},
	url = {https://doi.org/10.3847/1538-4365/aab766},
	year = 2018,
	month = {may},
	publisher = {American Astronomical Society},
	volume = {236},
	number = {1},
	pages = {16},
	author = {Jacob T. VanderPlas},
	title = {Understanding the Lomb{\textendash}Scargle Periodogram},
	journal = {The Astrophysical Journal Supplement Series},
	abstract = {The Lomb–Scargle periodogram is a well-known algorithm for detecting and characterizing periodic signals in unevenly sampled data. This paper presents a conceptual introduction to the Lomb–Scargle periodogram and important practical considerations for its use. Rather than a rigorous mathematical treatment, the goal of this paper is to build intuition about what assumptions are implicit in the use of the Lomb–Scargle periodogram and related estimators of periodicity, so as to motivate important practical considerations required in its proper application and interpretation.}
}
@misc{cabral2018fats,
      title={From FATS to feets: Further improvements to an astronomical feature extraction tool based on machine learning}, 
      author={J. B. Cabral and B. Sánchez and F. Ramos and S. Gurovich and P. Granitto and J. Vanderplas},
      year={2018},
      eprint={1809.02154},
      archivePrefix={arXiv},
      primaryClass={astro-ph.IM}
}

@article{Udalski,
author = {Udalski, A.},
year = {2004},
month = {01},
pages = {},
title = {The Optical Gravitational Lensing Experiment. Real time data analysis systems in the OGLE-III survey},
volume = {53},
journal = {Acta Astronomica - ACTA ASTRONOM}
}

@article{Udalski2,
author = {Udalski, A. and Szymański, M. and Szymański, G.},
year = {2015},
month = {04},
pages = {},
title = {OGLE-IV: Fourth phase of the Optical Gravitational Lensing Experiment},
volume = {65},
journal = {Acta Astronomica}
}

@article{ vizier,
	author = {{Ochsenbein, F.} and {Bauer, P.} and {Marcout, J.}},
	title = {The VizieR database of astronomical catalogues},
	DOI= "10.1051/aas:2000169",
	url= "https://doi.org/10.1051/aas:2000169",
	journal = {Astron. Astrophys. Suppl. Ser.},
	year = 2000,
	volume = 143,
	number = 1,
	pages = "23-32",
}

@misc{mcwilliam2011rr,
      title={RR Lyrae Stars, Metal-Poor Stars, and the Galaxy}, 
      author={Andrew McWilliam},
      year={2011},
      eprint={1109.1324},
      archivePrefix={arXiv},
      primaryClass={astro-ph.SR}
}

@article{cardelli,
author = {Cardelli, Jason and Clayton, Geoffrey and Mathis, John},
year = {1989},
month = {11},
pages = {},
title = {The relationship between infrared, optical, and ultraviolet extinction},
volume = {345},
journal = {The Astrophysical Journal},
doi = {10.1086/167900}
}

@article{Nishiyama,
author = {Nishiyama, Shogo and Tamura, Motohide and Hatano, Hirofumi and Kato, Daisuke and Tanabé, Toshihiko and Sugitani, Koji and Nagata, Tetsuya},
year = {2009},
month = {05},
pages = {1407-1417},
title = {Interstellar Extinction Law toward the Galactic Center III: J, H, Ks bands in the 2MASS and the MKO systems, and 3.6, 4.5, 5.8, 8.0 micron in the Spitzer/IRAC system},
volume = {696},
journal = {Astrophysical Journal - ASTROPHYS J},
doi = {10.1088/0004-637X/696/2/1407}
}
@article{Baade,
  title={A Search For the Nucleus of Our Galaxy},
  author={W. Baade},
  journal={Publications of the Astronomical Society of the Pacific},
  year={1946},
  volume={58},
  pages={249}
}

@book{mitchell,
  abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning---including probability and statistics, artificial intelligence, and neural networks---unifying them all in a logical and coherent manner.},
  added-at = {2017-05-08T14:37:30.000+0200},
  address = {New York},
  author = {Mitchell, Tom M.},
  biburl = {https://www.bibsonomy.org/bibtex/23e79734ee1a6e49aee02ffd108224d1c/flint63},
  file = {eBook:1900-99/Mitchell97.pdf:PDF;McGraw-Hill Product page:http\://www.mhprofessional.com/product.php?isbn=0070428077:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0070428077/:URL},
  groups = {public},
  interhash = {479a66c32badb3a455fbdcf8e6633a5d},
  intrahash = {3e79734ee1a6e49aee02ffd108224d1c},
  isbn = {978-0-07-042807-2},
  keywords = {01624 105 book shelf ai learn algorithm},
  publisher = {McGraw-Hill},
  timestamp = {2017-07-13T17:10:10.000+0200},
  title = {Machine Learning},
  username = {flint63},
  year = 1997
}

@MISC{carpynchoToolkit,
       author = {{Cabral}, Juan B. and {Ramos}, Felipe and {Gurovich}, Sebasti{\'a}n and {Granitto}, Pablo},
        title = "{Carpyncho: VVV Catalog browser toolkit}",
     keywords = {Software},
         year = 2020,
        month = may,
          eid = {ascl:2005.007},
        pages = {ascl:2005.007},
archivePrefix = {ascl},
       eprint = {2005.007},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020ascl.soft05007C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@conference{Pomerleau-1989-15721,
author = {Dean Pomerleau},
title = {ALVINN: An Autonomous Land Vehicle In a Neural Network},
booktitle = {Proceedings of Advances in Neural Information Processing Systems 1},
year = {1989},
month = {December},
editor = {D.S. Touretzky},
pages = {305 -313},
publisher = {Morgan Kaufmann},
}

@article{clasify_astronomy,
author = {Weir, Nick and Fayyad, Usama and Djorgovski, George},
year = {1995},
month = {05},
pages = {2401},
title = {Automated Star/Galaxy Classification for Digitized Poss-II},
volume = {109},
journal = {The Astronomical Journal},
doi = {10.1086/117459}
}

@Inbook{Tesauro1995,
author="Tesauro, Gerald",
title="TD-Gammon: A Self-Teaching Backgammon Program",
bookTitle="Applications of Neural Networks",
year="1995",
publisher="Springer US",
address="Boston, MA",
pages="267--285",
abstract="This chapter describes TD-Gammon, a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results. TD-Gammon uses a recently proposed reinforcement learning algorithm called TD($\lambda$) (Sutton, 1988), and is apparently the first application of this algorithm to a complex nontrivial task. Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e. given only a ``raw'' description of the board state), the network learns to play the entire game at a strong intermediate level that surpasses not only conventional commercial programs, but also comparable networks trained via supervised learning on a large corpus of human expert games. The hidden units in the network have apparently discovered useful features, a longstanding goal of computer games research.",
isbn="978-1-4757-2379-3",
doi="10.1007/978-1-4757-2379-3_11",
url="https://doi.org/10.1007/978-1-4757-2379-3_11"
}


@book{ai,
author = {Russell, Stuart and Norvig, Peter},
title = {Artificial Intelligence: A Modern Approach},
year = {2009},
isbn = {0136042597},
publisher = {Prentice Hall Press},
address = {USA},
edition = {3rd},
abstract = { The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence. }
}

@article{bagging,
author = {Breiman, Leo},
title = {Bagging Predictors},
year = {1996},
issue_date = {Aug. 1996},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1018054314350},
doi = {10.1023/A:1018054314350},
journal = {Mach. Learn.},
month = aug,
pages = {123–140},
numpages = {18},
keywords = {combining, averaging, bootstrap, aggregation}
}

@article{chi2,
  author = {Cochran, William G.},
  institution = {Institute of Mathematical Statistics},
  journal = {Annals of Mathematical Statistics},
  keywords = {statistics review},
  month = sep,
  number = 3,
  pages = {315--345},
  posted-at = {2014-02-04 10:51:40},
  priority = {2},
  timestamp = {2018-06-22T18:34:09.000+0200},
  title = {The chi-squared Test of Goodness of Fit},
  url = {http://www.jstor.org/stable/2236678},
  volume = 23,
  year = 1952
}

@book{meyer1970introductory,
  title={Introductory Probability and Statistical Applications},
  author={Meyer, P.L.},
  isbn={9780201047103},
  lccn={75104971},
  series={Addison-Wesley series in statistics},
  url={https://books.google.com.ar/books?id=99NQAAAAMAAJ},
  year={1970},
  publisher={Addison-Wesley Publishing Company}
}

@book{quinonero2009dataset,
  title={Dataset Shift in Machine Learning},
  author={Quinonero-Candela, J. and Sugiyama, M. and Lawrence, N.D. and Schwaighofer, A.},
  isbn={9780262170055},
  lccn={2008020394},
  series={Neural information processing series},
  url={https://books.google.com.ar/books?id=7zrhAAAAMAAJ},
  year={2009},
  publisher={MIT Press}
}

@article{MORENOTORRES2012521,
title = {A unifying view on dataset shift in classification},
journal = {Pattern Recognition},
volume = {45},
number = {1},
pages = {521-530},
year = {2012},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2011.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0031320311002901},
author = {Jose G. Moreno-Torres and Troy Raeder and Rocío Alaiz-Rodríguez and Nitesh V. Chawla and Francisco Herrera},
keywords = {Dataset shift, Data fracture, Changing environments, Differing training and test populations, Covariate shift, Sample selection bias, Non-stationary distributions},
abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature.}
}

@article{adaptative,
author = {Raza, Haider and Cecotti, Hubert and Li, Yuhua and Prasad, Girijesh},
year = {2016},
month = {08},
pages = {},
title = {Adaptive Learning with Covariate Shift-Detection for Motor Imagery based Brain-Computer Interface},
volume = {20},
journal = {Soft Computing},
doi = {10.1007/s00500-015-1937-5}
}

@misc{liu2017robust,
      title={Robust Covariate Shift Prediction with General Losses and Feature Views}, 
      author={Anqi Liu and Brian D. Ziebart},
      year={2017},
      eprint={1712.10043},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{widmer1998special,
  title={Special Issue on Context Sensitivity and Concept Drift},
  author={Widmer, G.},
  series={Machine learning},
  url={https://books.google.com.ar/books?id=8KU\_vwEACAAJ},
  year={1998},
  publisher={Kluwer}
}
@article{GeetaDharani2019CovariateSA,
  title={Covariate Shift: A Review and Analysis on Classifiers},
  author={Y GeetaDharani. and Nimisha G Nair and Pallavi Satpathy and J. Christopher},
  journal={2019 Global Conference for Advancement in Technology (GCAT)},
  year={2019},
  pages={1-6}
}

@book{non-stationary,
author = {Sugiyama, Masashi and Kawanabe, Motoaki},
title = {Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation},
year = {2012},
isbn = {0262017091},
publisher = {The MIT Press},
abstract = {As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.}
}
@article{LOPEZ20141,
title = {On the importance of the validation technique for classification with imbalanced datasets: Addressing covariate shift when data is skewed},
journal = {Information Sciences},
volume = {257},
pages = {1-13},
year = {2014},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2013.09.038},
url = {https://www.sciencedirect.com/science/article/pii/S0020025513006804},
author = {Victoria López and Alberto Fernández and Francisco Herrera},
keywords = {Classification, Imbalanced dataset, Covariate shift, Dataset shift, Validation technique, Partitioning},
abstract = {In the field of Data Mining, the estimation of the quality of the learned models is a key step in order to select the most appropriate tool for the problem to be solved. Traditionally, a k-fold validation technique has been carried out so that there is a certain degree of independency among the results for the different partitions. In this way, the highest average performance will be obtained by the most robust approach. However, applying a “random” division of the instances over the folds may result in a problem known as dataset shift, which consists in having a different data distribution between the training and test folds. In classification with imbalanced datasets, in which the number of instances of one class is much lower than the other class, this problem is more severe. The misclassification of minority class instances due to an incorrect learning of the real boundaries caused by a not well fitted data distribution, truly affects the measures of performance in this scenario. Regarding this fact, we propose the use of a specific validation technique for the partitioning of the data, known as “Distribution optimally balanced stratified cross-validation” to avoid this harmful situation in the presence of imbalance. This methodology makes the decision of placing close-by samples on different folds, so that each partition will end up with enough representatives of every region. We have selected a wide number of imbalanced datasets from KEEL dataset repository for our study, using several learning techniques from different paradigms, thus making the conclusions extracted to be independent of the underlying classifier. The analysis of the results has been carried out by means of the proper statistical study, which shows the goodness of this approach for dealing with imbalanced data.}
}

@article{10.5555/1314498.1390324,
author = {Sugiyama, Masashi and Krauledat, Matthias and M\"{u}ller, Klaus-Robert},
title = {Covariate Shift Adaptation by Importance Weighted Cross Validation},
year = {2007},
issue_date = {12/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {A common assumption in supervised learning is that the input points in the training set follow the same probability distribution as the input points that will be given in the future test phase. However, this assumption is not satisfied, for example, when the outside of the training region is extrapolated. The situation where the training input points and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one that can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {985–1005},
numpages = {21}
}


@inproceedings{adaptative_learning,
author = {Raza, Haider and Prasad, Girijesh and Li, Yuhua},
year = {2014},
month = {09},
pages = {1-8},
title = {Adaptive learning with covariate shift-detection for non-stationary environments},
journal = {2014 14th UK Workshop on Computational Intelligence, UKCI 2014 - Proceedings},
doi = {10.1109/UKCI.2014.6930161}
}

@misc{rabanser2019failing,
      title={Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift}, 
      author={Stephan Rabanser and Stephan Günnemann and Zachary C. Lipton},
      year={2019},
      eprint={1810.11953},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{kouw2019introduction,
      title={An introduction to domain adaptation and transfer learning}, 
      author={Wouter M. Kouw and Marco Loog},
      year={2019},
      eprint={1812.11806},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{raza,
author = {Raza, Haider and Cecotti, Hubert and Li, Yuhua and Prasad, Girijesh},
year = {2016},
month = {08},
pages = {},
title = {Adaptive Learning with Covariate Shift-Detection for Motor Imagery based Brain-Computer Interface},
volume = {20},
journal = {Soft Computing},
doi = {10.1007/s00500-015-1937-5}
}

@misc{wang2018extreme,
      title={Extreme Dimension Reduction for Handling Covariate Shift}, 
      author={Fulton Wang and Cynthia Rudin},
      year={2018},
      eprint={1711.10938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
  
@InProceedings{pmlr-v51-chen16d,
  title = 	 {Robust Covariate Shift Regression},
  author = 	 {Xiangli Chen and Mathew Monfort and Anqi Liu and Brian D. Ziebart},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1270--1279},
  year = 	 {2016},
  editor = 	 {Arthur Gretton and Christian C. Robert},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/chen16d.pdf},
  url = 	 {
http://proceedings.mlr.press/v51/chen16d.html
},
  abstract = 	 {In many learning settings, the source data available to train a regression model differs from the target data it encounters when making predictions due to input distribution shift. Appropriately dealing with this situation remains an important challenge. Existing methods attempt to “reweight” the source data samples to better represent the target domain, but this introduces strong inductive biases that are highly extrapolative and can often err greatly in practice. We propose a robust approach for regression under covariate shift that embraces the uncertainty resulting from sample selection bias by producing regression models that are explicitly robust to it. We demonstrate the benefits of our approach on a number of regression tasks.}
}

@ARTICLE{RePEc:hin:jnlmpe:302815,
title = {Training Classifiers under Covariate Shift by Constructing the Maximum Consistent Distribution Subset},
author = {Yu, Xu and Yu, Miao and Xu, Li-xun and Yang, Jing and Xie, Zhi-qiang},
year = {2015},
journal = {Mathematical Problems in Engineering},
volume = {2015},
pages = {1-9},
abstract = {The assumption that the training and testing samples are drawn from the same distribution is violated under covariate shift setting, and most algorithms for the covariate shift setting try to first estimate distributions and then reweight samples based on the distributions estimated. Due to the difficulty of estimating a correct distribution, previous methods can not get good classification performance. In this paper, we firstly present two types of covariate shift problems. Rather than estimating the distributions, we then desire an effective method to select a maximum subset following the target testing distribution based on feature space split from the auxiliary set or the target training set. Finally, we prove that our subset selection method can consistently deal with both scenarios of covariate shift. Experimental results demonstrate that training a classifier with the selected maximum subset exhibits good generalization ability and running efficiency over those of traditional methods under covariate shift setting.},
url = {https://EconPapers.repec.org/RePEc:hin:jnlmpe:302815}
}


@inproceedings{selectionbias,
author = {Zadrozny, Bianca},
title = {Learning and Evaluating Classifiers under Sample Selection Bias},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015425},
doi = {10.1145/1015330.1015425},
abstract = {Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {114},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{conceptdrift,
author = {Widmer, Gerhard and Kubat, M.},
year = {1994},
month = {11},
pages = {},
title = {Learning in the Presence of Concept Drift and Hidden Contexts},
volume = {23},
journal = {Machine Learning},
doi = {10.1007/BF00116900}
}

@inproceedings{changingenvironments,
author = {Alaiz, Rocío and Japkowicz, Nathalie},
year = {2008},
month = {05},
pages = {13-24},
title = {Assessing the Impact of Changing Environments on Classifier Performance},
volume = {5032},
isbn = {978-3-540-68821-1},
doi = {10.1007/978-3-540-68825-9_2}
}

@inproceedings{discriminative_distance,
author = {Bickel, Steffen and Br\"{u}ckner, Michael and Scheffer, Tobias},
title = {Discriminative Learning for Differing Training and Test Distributions},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273507},
doi = {10.1145/1273496.1273507},
abstract = {We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {81–88},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@book{molnar2019,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}

@article{noveltydetection,
author = {Cejnek, Matouš and Bukovsky, Ivo},
year = {2018},
month = {05},
pages = {},
title = {Concept drift robust adaptive novelty detection for data streams},
volume = {309},
journal = {Neurocomputing},
doi = {10.1016/j.neucom.2018.04.069}
}

